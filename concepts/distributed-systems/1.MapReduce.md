### MapReduce Concept
MapReduce is a system for parallelizing the computation of a large volume of data across multiple machines in a cluster. It achieves this by exposing a simple API for expressing these computations using two operations:
- **Map**: The Map task takes an input file and outputs a set of intermediate (key, value) pairs. 
- **Reduce**: The intermediate values with the same key are then grouped together and processed in the Reduce task for each distinct key.

##### The flow of execution:
- The MapReduce library splits the input into M files and starts up multiple instances of the program on many machines in the cluster.
- One of the instances is the Master and the others are Workers. The master assigns map tasks to the available workers.
- There are M map tasks and R reduce tasks to be assigned to workers.
- When a worker is assigned a map task, it reads the input from its corresponding split, performs the specified operations, and then emits intermediate (key, value) pairs which are buffered in memory.
- These buffered pairs are periodically written to local disk, partitioned into R regions according to the partitioning scheme (distributing key-value data across various nodes). The locations of these buffered pairs on local disk are passed to the master, which then forwards these locations to the reduce workers.
- A reduce worker uses remote procedure calls to read the buffered data from local disk based on the location it was forwarded by master. When all intermediate data has been read, it sorts the values according to the key and groups all occurrences for the same key together.
- For each distinct intermediate key, the reduce worker passes its grouped values to the reduce function defined. The output of this reduce function is appended to a final output file for the partition.
- The master wakes up the user program and releases control when all the map and reduce tasks have been completed.
The output of a MapReduce job is a set of R files (one per reduce task).

##### Fault Tolerance
- **Worker Failure**: The master pings all its workers periodically. If no response is received from a worker after a set period of time, the worker is marked as failed. The tasks assigned to the failed worker are then reassigned to other workers.
- **Master Failure**: As master failures were rare, their implementation simply aborted the whole execution if master failed, for the execution to be retried from the start. An alternative implementation could have made the master periodically checkpoint its state, so that a retry could pick up from where the execution left off.

##### Locality of Data
- Their implementation at the time, locality as a means of conserving network bandwidth. This means that the input files were kept close to where they will be processed to avoid the network trip of transferring these large files. 
- The Master's scheduling algorithm took the file location into account when determining what workers should execute what input files.
- Google's networking infrastructure was expanded and upgraded in later years, they relied less on this locality optimization.

##### Refinements in execution
 - **Partioning Function**: The users specify the number of reduce tasks/output files that they desire (R). Data gets partitioned across these tasks using a partitioning function on the intermediate key. A default partitioning function is provided that uses hashing (e.g. “hash(key) mod R”). This tends to result in fairly well-balanced partitions. In some cases, however, it is useful to partition data by some other function of the key. For example, sometimes the output keys are URLs, and we want all entries for a single host to end up in the same output file. To support situations like this, the user of the MapReduce library can provide a special partitioning function. For example, using “hash(Hostname(urlkey)) mod R” as the partitioning function causes all URLs from the same host to end up in the same output file.
 - **Ordering Guarantees**: The guarantee is that within a given reduce partition, the intermediate key value pairs are processed in increasing order of the keys.This lead to question where/how the sorting happens and it looks like in Hadoop MapReduce, data from the mappers are first sorted according to key in the mapper worker using Quicksort. This sorted data is then shuffled to the reduce workers. The reduce worker merges the sorted data from different mappers into one sorted set, similar to the merge routine in Mergesort.
 - **Combiner Function**: A user can specify a combiner function, which groups all the values for a key on a machine that performs a map task. The combiner function typically has the same code as the reduce function. The advantage of enabling this is that it can reduce the volume of the data being sent over the network to a reduce worker.
 - **Skipping Bad Records**: Instead of the entire execution failing because of few bad records, there's a mechanism in place for bad records to be skipped. This is acceptable in some instances e.g. when doing statistical analysis on a large dataset.
 - **For Debugging**: Local Execution Debugging problems in Map or Reduce functions can be tricky, since the actual computation happens in a distributed system, often on several thousand machines, with work assignment decisions made dynamically by the master. To help facilitate debugging, profiling, and small-scale testing, we have developed an alternative implementation of the MapReduce library that sequentially executes all of the work for a MapReduce operation on the local machine. Controls are provided to the user so that the computation can be limited to particular map tasks. Users invoke their program with a special flag and can then easily use any debugging or testing tools they find useful

#### References
- [Map Reduce Google Paper](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)
- [The Elephant was a Trojan Horse: On the Death of Map-Reduce at Google](https://www.the-paper-trail.org/post/2014-06-25-the-elephant-was-a-trojan-horse-on-the-death-of-map-reduce-at-google/)
- [Google Developers: Problem Solving on Large-Scale Clusters Lecture 1](https://www.youtube.com/watch?v=yjPBkvYh-ss)
- [Google Developers: Problem Solving on Large-Scale Clusters Lecture 2](https://www.youtube.com/watch?v=-vD6PUdf3Js)
- [Google Developers: Problem Solving on Large-Scale Clusters Lecture 3](https://www.youtube.com/watch?v=5Eib_H_zCEY)
- [Google Developers: Problem Solving on Large-Scale Clusters Lecture 4](https://www.youtube.com/watch?v=1ZDybXl212Q)
- [Google Developers: Problem Solving on Large-Scale Clusters Lecture 5](https://www.youtube.com/watch?v=BT-piFBP4fE)